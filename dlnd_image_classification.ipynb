{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 3 Max Value: 219\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 3 Name: cat\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxv43XI8p802nQmEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76da3c2128>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    #print(x/255.0)\n",
    "    return (x/255.0)\n",
    "    #np.array(x/255.0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     from sklearn import preprocessing\n",
    "#     labels = np.arange(10)\n",
    "#     lb = preprocessing.LabelBinarizer()\n",
    "#     lb.fit(labels)\n",
    "#     y = lb.transform(labels)\n",
    "#     print(y[x])\n",
    "    one_hot = np.eye(10)[x]\n",
    "    print(one_hot)\n",
    "    return one_hot\n",
    "    #return y[x]\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, image_shape[0], image_shape[1], image_shape[2]], name = 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, n_classes], name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Tensor(\"strided_slice:0\", shape=(32, 32, 5), dtype=float32)\n",
      "Tensor(\"Variable/read:0\", shape=(2, 2, 5, 10), dtype=float32)\n",
      "Tensor(\"Variable_1/read:0\", shape=(10,), dtype=float32)\n",
      "10 (2, 2) (4, 4) (2, 2) 2\n",
      "Tensor(\"Conv2D:0\", shape=(?, 8, 8, 10), dtype=float32)\n",
      "Tensor(\"BiasAdd:0\", shape=(?, 8, 8, 10), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 8, 8, 10), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 4, 4, 10), dtype=float32)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_channel_depth = int(x_tensor.get_shape()[3])\n",
    "    \n",
    "    print(input_channel_depth)\n",
    "    # 5\n",
    "    \n",
    "    print(x_tensor[1])\n",
    "    #Tensor(\"strided_slice:0\", shape=(32, 32, 5), dtype=float32)\n",
    "    #print(x_tensor)\n",
    "    #Tensor(\"Placeholder_18:0\", shape=(?, 32, 32, 5), dtype=float32)\n",
    "    \n",
    "    #weight = tf.Variable(tf.truncated_normal(\n",
    "    #[filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "    \n",
    "    filter_weights = tf.Variable(tf.truncated_normal((conv_ksize[0],conv_ksize[1],input_channel_depth,conv_num_outputs),mean=0.0, stddev=0.1,dtype=tf.float32))\n",
    "    print(filter_weights)\n",
    "    #Tensor(\"Variable_24/read:0\", shape=(2, 2, 5, 10), dtype=float32)\n",
    "    \n",
    "    filter_bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    print(filter_bias)\n",
    "    #Tensor(\"Variable_27/read:0\", shape=(10,), dtype=float32)\n",
    "    \n",
    "  \n",
    "    print(conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides[0])\n",
    "    #10 (2, 2) (4, 4) (2, 2) 22\n",
    "    \n",
    "    conv = tf.nn.conv2d(x_tensor, filter_weights, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME') \n",
    "    print(conv)\n",
    "    #Tensor(\"Conv2D_14:0\", shape=(?, 16, 16, 10), dtype=float32)\n",
    "    \n",
    "    conv = tf.nn.bias_add(conv, filter_bias)\n",
    "    print(conv)\n",
    "    #Tensor(\"BiasAdd_10:0\", shape=(?, 16, 16, 10), dtype=float32)\n",
    "    \n",
    "    conv = tf.nn.relu(conv)\n",
    "    print(conv)\n",
    "    #Tensor(\"Relu_11:0\", shape=(?, 16, 16, 10), dtype=float32)\n",
    "    \n",
    "    conv = tf.nn.max_pool(conv,ksize=[1,pool_ksize[0],pool_ksize[1],1], strides=[1, pool_strides[0],pool_strides[1], 1], padding='SAME')\n",
    "    print(conv)\n",
    "    #Tensor(\"MaxPool_11:0\", shape=(?, 8, 8, 10), dtype=float32)\n",
    "   \n",
    "    return conv \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.reshape(x_tensor,[-1, x_tensor.get_shape().as_list()[1] * x_tensor.get_shape().as_list()[2] * x_tensor.get_shape().as_list()[3]])\n",
    "    #tf.reshape(x_tensor, [-1, x_tensor.shape[1] * x_tensor.shape[2] * x_tensor.shape[3]])\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable_2/read:0\", shape=(128, 40), dtype=float32)\n",
      "Tensor(\"Variable_3/read:0\", shape=(40,), dtype=float32)\n",
      "Tensor(\"Placeholder_2:0\", shape=(?, 128), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 40), dtype=float32)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal((x_tensor.get_shape().as_list()[1],num_outputs),mean=0.0, stddev=0.01))\n",
    "    print(weights)\n",
    "    #Tensor(\"Variable_68/read:0\", shape=(40, 128), dtype=float32)\n",
    "    \n",
    "    biases = tf.Variable(tf.zeros([num_outputs]))\n",
    "    print(biases)\n",
    "    #Tensor(\"Variable_71/read:0\", shape=(40,), dtype=float32)\n",
    "    \n",
    "    print(x_tensor)\n",
    "    #Tensor(\"Placeholder_48:0\", shape=(?, 128), dtype=float32)\n",
    "    #tf.cast(x_tensor, tf.int32)\n",
    "    #x_tensor = tf.to_int32(x_tensor)\n",
    "    #layer = tf.matmul(x_tensor, num_outputs) + biases\n",
    "    layer = tf.add(tf.matmul( x_tensor,weights),biases)\n",
    "    #Tensor(\"MatMul_9:0\", shape=(40, 128), dtype=float32)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    print(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "40\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    print(x_tensor.get_shape()[-1])\n",
    "    #?\n",
    "    print(num_outputs)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[-1],num_outputs],mean=0.0, stddev=0.01))\n",
    "    biases = tf.Variable(tf.zeros([num_outputs]))\n",
    "    output = tf.nn.bias_add(tf.matmul(x_tensor,weights),biases)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Tensor(\"strided_slice:0\", shape=(32, 32, 3), dtype=float32)\n",
      "Tensor(\"Variable/read:0\", shape=(5, 5, 3, 16), dtype=float32)\n",
      "Tensor(\"Variable_1/read:0\", shape=(16,), dtype=float32)\n",
      "16 (5, 5) (1, 1) (2, 2) 2\n",
      "Tensor(\"Conv2D:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "Tensor(\"BiasAdd:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 16, 16, 16), dtype=float32)\n",
      "16\n",
      "Tensor(\"strided_slice_1:0\", shape=(16, 16, 16), dtype=float32)\n",
      "Tensor(\"Variable_2/read:0\", shape=(4, 4, 16, 32), dtype=float32)\n",
      "Tensor(\"Variable_3/read:0\", shape=(32,), dtype=float32)\n",
      "32 (4, 4) (1, 1) (1, 1) 1\n",
      "Tensor(\"Conv2D_1:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "Tensor(\"BiasAdd_1:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "32\n",
      "Tensor(\"strided_slice_2:0\", shape=(16, 16, 32), dtype=float32)\n",
      "Tensor(\"Variable_4/read:0\", shape=(4, 4, 32, 64), dtype=float32)\n",
      "Tensor(\"Variable_5/read:0\", shape=(64,), dtype=float32)\n",
      "64 (4, 4) (1, 1) (2, 2) 2\n",
      "Tensor(\"Conv2D_2:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"BiasAdd_2:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"MaxPool_2:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"Variable_6/read:0\", shape=(4096, 50), dtype=float32)\n",
      "Tensor(\"Variable_7/read:0\", shape=(50,), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 4096), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 50), dtype=float32)\n",
      "50\n",
      "10\n",
      "3\n",
      "Tensor(\"strided_slice_3:0\", shape=(32, 32, 3), dtype=float32)\n",
      "Tensor(\"Variable_10/read:0\", shape=(5, 5, 3, 16), dtype=float32)\n",
      "Tensor(\"Variable_11/read:0\", shape=(16,), dtype=float32)\n",
      "16 (5, 5) (1, 1) (2, 2) 2\n",
      "Tensor(\"Conv2D_3:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "Tensor(\"BiasAdd_4:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "Tensor(\"Relu_4:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_3:0\", shape=(?, 16, 16, 16), dtype=float32)\n",
      "16\n",
      "Tensor(\"strided_slice_4:0\", shape=(16, 16, 16), dtype=float32)\n",
      "Tensor(\"Variable_12/read:0\", shape=(4, 4, 16, 32), dtype=float32)\n",
      "Tensor(\"Variable_13/read:0\", shape=(32,), dtype=float32)\n",
      "32 (4, 4) (1, 1) (1, 1) 1\n",
      "Tensor(\"Conv2D_4:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "Tensor(\"BiasAdd_5:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "Tensor(\"Relu_5:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "Tensor(\"MaxPool_4:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "32\n",
      "Tensor(\"strided_slice_5:0\", shape=(16, 16, 32), dtype=float32)\n",
      "Tensor(\"Variable_14/read:0\", shape=(4, 4, 32, 64), dtype=float32)\n",
      "Tensor(\"Variable_15/read:0\", shape=(64,), dtype=float32)\n",
      "64 (4, 4) (1, 1) (2, 2) 2\n",
      "Tensor(\"Conv2D_5:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"BiasAdd_6:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"Relu_6:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"MaxPool_5:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"Variable_16/read:0\", shape=(4096, 50), dtype=float32)\n",
      "Tensor(\"Variable_17/read:0\", shape=(50,), dtype=float32)\n",
      "Tensor(\"Reshape_4:0\", shape=(?, 4096), dtype=float32)\n",
      "Tensor(\"Relu_7:0\", shape=(?, 50), dtype=float32)\n",
      "50\n",
      "10\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "#     conv_strides = (1,1)\n",
    "#     conv_ksize = (4,4)\n",
    "#     conv_num_outputs = 10\n",
    "#     pool_ksize = (2,2)\n",
    "#     pool_strides = (1,1)\n",
    "#     num_outputs = 10\n",
    "    \n",
    "    \n",
    "    #conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #   flatten(x_tensor)\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    layer = conv2d_maxpool(x, 16,(5,5),(1,1),(2,2),(2,2))\n",
    "    \n",
    "    \n",
    "    layer = conv2d_maxpool(layer, 32,(4,4),(1,1),(1,1),(1,1))\n",
    "    layer = conv2d_maxpool(layer, 64,(4,4),(1,1),(2,2),(2,2))\n",
    "    layer = flatten(layer)\n",
    "    \n",
    "    layer = fully_conn(layer,50)\n",
    "    layer = tf.nn.dropout(layer,keep_prob)\n",
    "    layer = output(layer,10)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     loss = session.run(cost,feed_dict={x:feature_batch, y: label_batch,keep_prob:0.5})\n",
    "#     accuracy = session.run(accuracy, feed_dict={x:valid_features, y:valid_labels, keep_prob: 0.5})\n",
    "    \n",
    "#     print(\"loss: {}\".format(loss))\n",
    "#     print(\"accuracy: {}\".format(accuracy))\n",
    "    \n",
    "    print (\"Loss:\", session.run(cost,feed_dict={x: feature_batch,y: label_batch,keep_prob: 1.0}),\n",
    "           \" Train-Accu:\", session.run(accuracy, feed_dict={x: feature_batch,y: label_batch,keep_prob: 1.0}),\n",
    "           \" Valid-Accu:\", session.run(accuracy, feed_dict={x: valid_features,y: valid_labels,keep_prob: 1.0}))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size =256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.21781  Train-Accu: 0.225  Valid-Accu: 0.2302\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.14375  Train-Accu: 0.2  Valid-Accu: 0.2758\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.10159  Train-Accu: 0.2  Valid-Accu: 0.306\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.95833  Train-Accu: 0.425  Valid-Accu: 0.3574\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.96731  Train-Accu: 0.35  Valid-Accu: 0.3474\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.83704  Train-Accu: 0.375  Valid-Accu: 0.3784\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.74843  Train-Accu: 0.525  Valid-Accu: 0.4024\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.76086  Train-Accu: 0.425  Valid-Accu: 0.3808\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.63091  Train-Accu: 0.425  Valid-Accu: 0.3942\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.54987  Train-Accu: 0.475  Valid-Accu: 0.408\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.52824  Train-Accu: 0.55  Valid-Accu: 0.4144\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.47597  Train-Accu: 0.525  Valid-Accu: 0.4264\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.46673  Train-Accu: 0.5  Valid-Accu: 0.4286\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.35106  Train-Accu: 0.6  Valid-Accu: 0.421\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.31828  Train-Accu: 0.575  Valid-Accu: 0.423\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.27265  Train-Accu: 0.5  Valid-Accu: 0.436\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.27412  Train-Accu: 0.575  Valid-Accu: 0.4366\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.1786  Train-Accu: 0.675  Valid-Accu: 0.4652\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.171  Train-Accu: 0.65  Valid-Accu: 0.4578\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.09229  Train-Accu: 0.775  Valid-Accu: 0.4602\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.07733  Train-Accu: 0.725  Valid-Accu: 0.4644\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.02584  Train-Accu: 0.7  Valid-Accu: 0.4816\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.972458  Train-Accu: 0.725  Valid-Accu: 0.482\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.965688  Train-Accu: 0.775  Valid-Accu: 0.4842\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.938777  Train-Accu: 0.775  Valid-Accu: 0.483\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.897713  Train-Accu: 0.725  Valid-Accu: 0.4906\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.889399  Train-Accu: 0.75  Valid-Accu: 0.4972\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.884088  Train-Accu: 0.75  Valid-Accu: 0.4812\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.820123  Train-Accu: 0.8  Valid-Accu: 0.4838\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.827941  Train-Accu: 0.75  Valid-Accu: 0.4994\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.782868  Train-Accu: 0.775  Valid-Accu: 0.4984\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.761681  Train-Accu: 0.775  Valid-Accu: 0.4912\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.744744  Train-Accu: 0.725  Valid-Accu: 0.508\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.733721  Train-Accu: 0.8  Valid-Accu: 0.517\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.69172  Train-Accu: 0.8  Valid-Accu: 0.5168\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.745439  Train-Accu: 0.75  Valid-Accu: 0.498\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.722762  Train-Accu: 0.775  Valid-Accu: 0.5112\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.630009  Train-Accu: 0.8  Valid-Accu: 0.5242\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.629815  Train-Accu: 0.825  Valid-Accu: 0.5302\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.62325  Train-Accu: 0.8  Valid-Accu: 0.5238\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.624755  Train-Accu: 0.85  Valid-Accu: 0.5206\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.552534  Train-Accu: 0.8  Valid-Accu: 0.5382\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.527331  Train-Accu: 0.85  Valid-Accu: 0.547\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.523803  Train-Accu: 0.8  Valid-Accu: 0.5504\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.505271  Train-Accu: 0.875  Valid-Accu: 0.547\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.473084  Train-Accu: 0.85  Valid-Accu: 0.5484\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.454332  Train-Accu: 0.875  Valid-Accu: 0.5506\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.451096  Train-Accu: 0.9  Valid-Accu: 0.5548\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.448284  Train-Accu: 0.85  Valid-Accu: 0.5512\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.433758  Train-Accu: 0.9  Valid-Accu: 0.5476\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.429804  Train-Accu: 0.925  Valid-Accu: 0.5502\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.385174  Train-Accu: 0.875  Valid-Accu: 0.5546\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.394341  Train-Accu: 0.9  Valid-Accu: 0.5498\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.40351  Train-Accu: 0.925  Valid-Accu: 0.5512\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.367585  Train-Accu: 0.9  Valid-Accu: 0.558\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.403403  Train-Accu: 0.95  Valid-Accu: 0.551\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.438629  Train-Accu: 0.9  Valid-Accu: 0.539\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.384194  Train-Accu: 0.95  Valid-Accu: 0.5508\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.380254  Train-Accu: 0.9  Valid-Accu: 0.5362\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.352582  Train-Accu: 0.925  Valid-Accu: 0.551\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.347282  Train-Accu: 0.925  Valid-Accu: 0.5394\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.362695  Train-Accu: 0.95  Valid-Accu: 0.5356\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.39384  Train-Accu: 0.925  Valid-Accu: 0.5188\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.356128  Train-Accu: 0.925  Valid-Accu: 0.54\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.317765  Train-Accu: 0.925  Valid-Accu: 0.5542\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.290638  Train-Accu: 0.95  Valid-Accu: 0.5556\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.302854  Train-Accu: 0.875  Valid-Accu: 0.55\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.25384  Train-Accu: 0.9  Valid-Accu: 0.5548\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.277073  Train-Accu: 0.95  Valid-Accu: 0.5556\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.338352  Train-Accu: 0.925  Valid-Accu: 0.534\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.293332  Train-Accu: 0.9  Valid-Accu: 0.544\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.354114  Train-Accu: 0.925  Valid-Accu: 0.5262\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.29342  Train-Accu: 0.925  Valid-Accu: 0.5378\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.285349  Train-Accu: 0.95  Valid-Accu: 0.5466\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.267288  Train-Accu: 0.95  Valid-Accu: 0.5486\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.263128  Train-Accu: 0.95  Valid-Accu: 0.5466\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.264023  Train-Accu: 0.95  Valid-Accu: 0.5514\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.257251  Train-Accu: 0.975  Valid-Accu: 0.5498\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.242799  Train-Accu: 0.95  Valid-Accu: 0.5484\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.245374  Train-Accu: 0.95  Valid-Accu: 0.5488\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.225152  Train-Accu: 0.95  Valid-Accu: 0.555\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.237074  Train-Accu: 0.975  Valid-Accu: 0.548\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.22147  Train-Accu: 1.0  Valid-Accu: 0.5564\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.237527  Train-Accu: 0.925  Valid-Accu: 0.553\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.2101  Train-Accu: 0.95  Valid-Accu: 0.56\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.214405  Train-Accu: 0.925  Valid-Accu: 0.5522\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.214225  Train-Accu: 0.95  Valid-Accu: 0.5516\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.228107  Train-Accu: 0.925  Valid-Accu: 0.5458\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.204605  Train-Accu: 1.0  Valid-Accu: 0.5506\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.218637  Train-Accu: 0.95  Valid-Accu: 0.5554\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.196055  Train-Accu: 0.925  Valid-Accu: 0.5614\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.185253  Train-Accu: 0.975  Valid-Accu: 0.5644\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.175383  Train-Accu: 0.975  Valid-Accu: 0.5596\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.169825  Train-Accu: 1.0  Valid-Accu: 0.5598\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.182592  Train-Accu: 1.0  Valid-Accu: 0.5428\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.167521  Train-Accu: 0.95  Valid-Accu: 0.5546\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.17829  Train-Accu: 1.0  Valid-Accu: 0.5358\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.178281  Train-Accu: 0.975  Valid-Accu: 0.5546\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.165197  Train-Accu: 1.0  Valid-Accu: 0.5512\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.168316  Train-Accu: 0.975  Valid-Accu: 0.5522\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.22135  Train-Accu: 0.1  Valid-Accu: 0.2132\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.17843  Train-Accu: 0.075  Valid-Accu: 0.1914\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 1.79003  Train-Accu: 0.25  Valid-Accu: 0.2498\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 1.81763  Train-Accu: 0.225  Valid-Accu: 0.3364\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.78935  Train-Accu: 0.325  Valid-Accu: 0.3548\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.98883  Train-Accu: 0.35  Valid-Accu: 0.3798\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.75677  Train-Accu: 0.45  Valid-Accu: 0.3866\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.57655  Train-Accu: 0.375  Valid-Accu: 0.37\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.70044  Train-Accu: 0.375  Valid-Accu: 0.394\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.66857  Train-Accu: 0.325  Valid-Accu: 0.4056\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.83086  Train-Accu: 0.4  Valid-Accu: 0.416\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.6539  Train-Accu: 0.45  Valid-Accu: 0.4174\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.40892  Train-Accu: 0.425  Valid-Accu: 0.4272\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.6322  Train-Accu: 0.4  Valid-Accu: 0.4258\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.59632  Train-Accu: 0.475  Valid-Accu: 0.4348\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.71544  Train-Accu: 0.525  Valid-Accu: 0.443\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.5909  Train-Accu: 0.5  Valid-Accu: 0.458\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.29208  Train-Accu: 0.45  Valid-Accu: 0.4492\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.5608  Train-Accu: 0.45  Valid-Accu: 0.4312\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.51508  Train-Accu: 0.45  Valid-Accu: 0.4694\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.58877  Train-Accu: 0.475  Valid-Accu: 0.4646\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.5268  Train-Accu: 0.525  Valid-Accu: 0.457\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.20913  Train-Accu: 0.625  Valid-Accu: 0.49\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.50416  Train-Accu: 0.475  Valid-Accu: 0.4834\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.48799  Train-Accu: 0.475  Valid-Accu: 0.4796\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.49031  Train-Accu: 0.525  Valid-Accu: 0.5028\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.37499  Train-Accu: 0.4  Valid-Accu: 0.5032\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.16579  Train-Accu: 0.6  Valid-Accu: 0.5072\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.46219  Train-Accu: 0.525  Valid-Accu: 0.5138\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.39829  Train-Accu: 0.55  Valid-Accu: 0.5046\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.33114  Train-Accu: 0.6  Valid-Accu: 0.512\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.35626  Train-Accu: 0.5  Valid-Accu: 0.4762\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.12158  Train-Accu: 0.625  Valid-Accu: 0.5246\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.39375  Train-Accu: 0.55  Valid-Accu: 0.5348\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.22851  Train-Accu: 0.65  Valid-Accu: 0.5454\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.26977  Train-Accu: 0.5  Valid-Accu: 0.5508\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.2524  Train-Accu: 0.575  Valid-Accu: 0.5358\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.02453  Train-Accu: 0.625  Valid-Accu: 0.5324\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.24934  Train-Accu: 0.6  Valid-Accu: 0.5544\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.17806  Train-Accu: 0.65  Valid-Accu: 0.5394\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.21421  Train-Accu: 0.575  Valid-Accu: 0.5612\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.11253  Train-Accu: 0.6  Valid-Accu: 0.5222\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 0.965478  Train-Accu: 0.725  Valid-Accu: 0.5554\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.21048  Train-Accu: 0.625  Valid-Accu: 0.569\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.12112  Train-Accu: 0.65  Valid-Accu: 0.5688\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.12414  Train-Accu: 0.6  Valid-Accu: 0.5588\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.07587  Train-Accu: 0.55  Valid-Accu: 0.5742\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 0.9416  Train-Accu: 0.675  Valid-Accu: 0.5762\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.06857  Train-Accu: 0.65  Valid-Accu: 0.5746\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.05337  Train-Accu: 0.7  Valid-Accu: 0.5786\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.00808  Train-Accu: 0.625  Valid-Accu: 0.5944\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 0.984325  Train-Accu: 0.575  Valid-Accu: 0.6002\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 0.854722  Train-Accu: 0.775  Valid-Accu: 0.5862\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.00593  Train-Accu: 0.675  Valid-Accu: 0.5964\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 0.97336  Train-Accu: 0.775  Valid-Accu: 0.6008\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 0.982928  Train-Accu: 0.65  Valid-Accu: 0.6084\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 0.966754  Train-Accu: 0.625  Valid-Accu: 0.5902\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 0.786397  Train-Accu: 0.825  Valid-Accu: 0.6026\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 0.953238  Train-Accu: 0.725  Valid-Accu: 0.6036\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 0.902213  Train-Accu: 0.75  Valid-Accu: 0.6018\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 0.892479  Train-Accu: 0.7  Valid-Accu: 0.5984\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 0.871901  Train-Accu: 0.65  Valid-Accu: 0.601\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 0.687538  Train-Accu: 0.75  Valid-Accu: 0.6068\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 0.892644  Train-Accu: 0.65  Valid-Accu: 0.5984\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 0.854446  Train-Accu: 0.775  Valid-Accu: 0.6126\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 0.818155  Train-Accu: 0.75  Valid-Accu: 0.6084\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 0.794832  Train-Accu: 0.675  Valid-Accu: 0.6194\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.675599  Train-Accu: 0.775  Valid-Accu: 0.616\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 0.846324  Train-Accu: 0.725  Valid-Accu: 0.6206\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 0.808787  Train-Accu: 0.875  Valid-Accu: 0.6194\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 0.769231  Train-Accu: 0.8  Valid-Accu: 0.6142\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 0.737699  Train-Accu: 0.7  Valid-Accu: 0.6176\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 0.610103  Train-Accu: 0.775  Valid-Accu: 0.616\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 0.781856  Train-Accu: 0.725  Valid-Accu: 0.6202\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 0.704627  Train-Accu: 0.875  Valid-Accu: 0.6354\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 0.750237  Train-Accu: 0.775  Valid-Accu: 0.619\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 0.674518  Train-Accu: 0.725  Valid-Accu: 0.6348\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 0.585644  Train-Accu: 0.85  Valid-Accu: 0.6222\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 0.691457  Train-Accu: 0.75  Valid-Accu: 0.6332\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 0.680996  Train-Accu: 0.925  Valid-Accu: 0.6322\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 0.709785  Train-Accu: 0.8  Valid-Accu: 0.6194\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 0.643447  Train-Accu: 0.75  Valid-Accu: 0.6362\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.516565  Train-Accu: 0.85  Valid-Accu: 0.6262\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 0.744942  Train-Accu: 0.675  Valid-Accu: 0.6314\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 0.616508  Train-Accu: 0.9  Valid-Accu: 0.6396\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.676182  Train-Accu: 0.8  Valid-Accu: 0.6052\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 0.643894  Train-Accu: 0.775  Valid-Accu: 0.6316\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.505496  Train-Accu: 0.85  Valid-Accu: 0.6366\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 0.652356  Train-Accu: 0.75  Valid-Accu: 0.6462\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.601025  Train-Accu: 0.875  Valid-Accu: 0.6406\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.600361  Train-Accu: 0.775  Valid-Accu: 0.6504\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 0.589168  Train-Accu: 0.725  Valid-Accu: 0.641\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.506809  Train-Accu: 0.875  Valid-Accu: 0.6346\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 0.670353  Train-Accu: 0.85  Valid-Accu: 0.6462\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.558365  Train-Accu: 0.925  Valid-Accu: 0.6502\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.559714  Train-Accu: 0.85  Valid-Accu: 0.639\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 0.572074  Train-Accu: 0.75  Valid-Accu: 0.6384\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.467978  Train-Accu: 0.85  Valid-Accu: 0.6504\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 0.58649  Train-Accu: 0.875  Valid-Accu: 0.6592\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.539904  Train-Accu: 0.925  Valid-Accu: 0.6592\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.53751  Train-Accu: 0.825  Valid-Accu: 0.6482\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 0.540642  Train-Accu: 0.8  Valid-Accu: 0.6338\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.46478  Train-Accu: 0.875  Valid-Accu: 0.6486\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.551229  Train-Accu: 0.875  Valid-Accu: 0.6544\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.460173  Train-Accu: 0.925  Valid-Accu: 0.6554\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.528419  Train-Accu: 0.875  Valid-Accu: 0.6586\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 0.533333  Train-Accu: 0.875  Valid-Accu: 0.636\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.426831  Train-Accu: 0.875  Valid-Accu: 0.655\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.518964  Train-Accu: 0.9  Valid-Accu: 0.6648\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.462182  Train-Accu: 0.95  Valid-Accu: 0.6608\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.479373  Train-Accu: 0.9  Valid-Accu: 0.6578\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.490267  Train-Accu: 0.8  Valid-Accu: 0.6472\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.426599  Train-Accu: 0.925  Valid-Accu: 0.6546\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.547395  Train-Accu: 0.775  Valid-Accu: 0.6632\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.467384  Train-Accu: 0.925  Valid-Accu: 0.6654\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.438657  Train-Accu: 0.85  Valid-Accu: 0.66\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.501313  Train-Accu: 0.85  Valid-Accu: 0.6432\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.363609  Train-Accu: 0.875  Valid-Accu: 0.6594\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.496144  Train-Accu: 0.9  Valid-Accu: 0.6688\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.427807  Train-Accu: 0.925  Valid-Accu: 0.6674\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.441691  Train-Accu: 0.875  Valid-Accu: 0.6568\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.444533  Train-Accu: 0.825  Valid-Accu: 0.6514\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.402334  Train-Accu: 0.925  Valid-Accu: 0.649\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.517076  Train-Accu: 0.875  Valid-Accu: 0.663\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.453921  Train-Accu: 0.95  Valid-Accu: 0.6682\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.455224  Train-Accu: 0.925  Valid-Accu: 0.671\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.443641  Train-Accu: 0.85  Valid-Accu: 0.6718\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.402485  Train-Accu: 0.95  Valid-Accu: 0.6672\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.439536  Train-Accu: 0.9  Valid-Accu: 0.6794\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.392672  Train-Accu: 0.975  Valid-Accu: 0.668\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.401191  Train-Accu: 0.875  Valid-Accu: 0.67\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.428167  Train-Accu: 0.8  Valid-Accu: 0.663\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.342797  Train-Accu: 0.9  Valid-Accu: 0.6646\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.42995  Train-Accu: 0.9  Valid-Accu: 0.6652\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.390302  Train-Accu: 0.95  Valid-Accu: 0.6686\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.409854  Train-Accu: 0.875  Valid-Accu: 0.6682\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.416008  Train-Accu: 0.875  Valid-Accu: 0.6686\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.298644  Train-Accu: 0.9  Valid-Accu: 0.671\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.423367  Train-Accu: 0.825  Valid-Accu: 0.6782\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.35555  Train-Accu: 0.975  Valid-Accu: 0.6778\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.358217  Train-Accu: 0.95  Valid-Accu: 0.6646\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.390487  Train-Accu: 0.875  Valid-Accu: 0.6692\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.320903  Train-Accu: 0.95  Valid-Accu: 0.6664\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.390392  Train-Accu: 0.9  Valid-Accu: 0.6784\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.348961  Train-Accu: 0.95  Valid-Accu: 0.6722\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.368646  Train-Accu: 0.9  Valid-Accu: 0.6626\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.382872  Train-Accu: 0.85  Valid-Accu: 0.6658\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.298268  Train-Accu: 0.95  Valid-Accu: 0.674\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.402944  Train-Accu: 0.925  Valid-Accu: 0.672\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.341084  Train-Accu: 0.925  Valid-Accu: 0.6808\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.337826  Train-Accu: 0.95  Valid-Accu: 0.6688\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.347803  Train-Accu: 0.85  Valid-Accu: 0.6716\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.321069  Train-Accu: 0.925  Valid-Accu: 0.68\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.372459  Train-Accu: 0.925  Valid-Accu: 0.6728\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.323848  Train-Accu: 0.975  Valid-Accu: 0.6692\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.342187  Train-Accu: 0.9  Valid-Accu: 0.6766\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.341623  Train-Accu: 0.875  Valid-Accu: 0.6588\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.273231  Train-Accu: 0.9  Valid-Accu: 0.6772\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.335497  Train-Accu: 0.925  Valid-Accu: 0.6886\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.32364  Train-Accu: 0.975  Valid-Accu: 0.6584\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.336747  Train-Accu: 0.925  Valid-Accu: 0.6818\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.32558  Train-Accu: 0.925  Valid-Accu: 0.682\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.271566  Train-Accu: 0.95  Valid-Accu: 0.6802\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.326315  Train-Accu: 0.9  Valid-Accu: 0.685\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.267484  Train-Accu: 0.975  Valid-Accu: 0.6858\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.320101  Train-Accu: 0.9  Valid-Accu: 0.6812\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.321186  Train-Accu: 0.925  Valid-Accu: 0.685\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.279613  Train-Accu: 0.95  Valid-Accu: 0.6778\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.320381  Train-Accu: 0.95  Valid-Accu: 0.6858\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.260373  Train-Accu: 0.975  Valid-Accu: 0.669\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.295152  Train-Accu: 0.925  Valid-Accu: 0.6838\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.321717  Train-Accu: 0.9  Valid-Accu: 0.6834\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.279903  Train-Accu: 0.925  Valid-Accu: 0.6756\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.304532  Train-Accu: 0.975  Valid-Accu: 0.6876\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.224385  Train-Accu: 0.975  Valid-Accu: 0.6764\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.304489  Train-Accu: 0.925  Valid-Accu: 0.6808\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.285865  Train-Accu: 0.875  Valid-Accu: 0.6758\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.27648  Train-Accu: 0.975  Valid-Accu: 0.6838\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.320197  Train-Accu: 0.9  Valid-Accu: 0.6792\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.24982  Train-Accu: 0.975  Valid-Accu: 0.6804\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.292929  Train-Accu: 0.925  Valid-Accu: 0.6794\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.29189  Train-Accu: 0.925  Valid-Accu: 0.6774\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.261241  Train-Accu: 0.975  Valid-Accu: 0.6732\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.298497  Train-Accu: 0.975  Valid-Accu: 0.6882\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.217461  Train-Accu: 0.975  Valid-Accu: 0.6774\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.305042  Train-Accu: 0.95  Valid-Accu: 0.6684\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.275161  Train-Accu: 0.95  Valid-Accu: 0.6792\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.24524  Train-Accu: 0.925  Valid-Accu: 0.6556\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.301991  Train-Accu: 0.95  Valid-Accu: 0.6734\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.221623  Train-Accu: 0.975  Valid-Accu: 0.6736\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.309682  Train-Accu: 0.95  Valid-Accu: 0.674\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.29946  Train-Accu: 0.9  Valid-Accu: 0.6692\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.226411  Train-Accu: 0.95  Valid-Accu: 0.6724\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.276349  Train-Accu: 0.95  Valid-Accu: 0.6856\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.224888  Train-Accu: 0.975  Valid-Accu: 0.6804\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.27024  Train-Accu: 1.0  Valid-Accu: 0.6866\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.286673  Train-Accu: 0.925  Valid-Accu: 0.6772\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.216251  Train-Accu: 0.975  Valid-Accu: 0.6574\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.290831  Train-Accu: 0.95  Valid-Accu: 0.6822\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.209963  Train-Accu: 0.975  Valid-Accu: 0.6746\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.32822  Train-Accu: 0.9  Valid-Accu: 0.6742\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.356259  Train-Accu: 0.925  Valid-Accu: 0.6714\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.227929  Train-Accu: 0.975  Valid-Accu: 0.6678\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.279086  Train-Accu: 0.95  Valid-Accu: 0.6828\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.208718  Train-Accu: 0.975  Valid-Accu: 0.683\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.278278  Train-Accu: 0.925  Valid-Accu: 0.6782\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.286782  Train-Accu: 0.95  Valid-Accu: 0.6724\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.216288  Train-Accu: 1.0  Valid-Accu: 0.6678\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.293119  Train-Accu: 0.95  Valid-Accu: 0.6838\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.191061  Train-Accu: 0.975  Valid-Accu: 0.676\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.268734  Train-Accu: 0.975  Valid-Accu: 0.6846\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.273767  Train-Accu: 0.9  Valid-Accu: 0.675\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.217239  Train-Accu: 0.975  Valid-Accu: 0.669\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.273239  Train-Accu: 0.95  Valid-Accu: 0.686\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.199781  Train-Accu: 0.975  Valid-Accu: 0.6798\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.270944  Train-Accu: 0.975  Valid-Accu: 0.6846\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.264047  Train-Accu: 0.925  Valid-Accu: 0.6736\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.240836  Train-Accu: 0.95  Valid-Accu: 0.6624\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.286271  Train-Accu: 0.95  Valid-Accu: 0.6768\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.192968  Train-Accu: 0.975  Valid-Accu: 0.67\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.257478  Train-Accu: 0.975  Valid-Accu: 0.6826\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.247295  Train-Accu: 0.925  Valid-Accu: 0.6666\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.210658  Train-Accu: 0.975  Valid-Accu: 0.6688\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.281426  Train-Accu: 0.95  Valid-Accu: 0.6828\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.180157  Train-Accu: 0.975  Valid-Accu: 0.6852\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.229222  Train-Accu: 0.975  Valid-Accu: 0.6846\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.260753  Train-Accu: 0.925  Valid-Accu: 0.6558\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.214473  Train-Accu: 0.975  Valid-Accu: 0.6704\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.28396  Train-Accu: 0.95  Valid-Accu: 0.6796\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.207108  Train-Accu: 0.975  Valid-Accu: 0.6712\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.241086  Train-Accu: 0.95  Valid-Accu: 0.673\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.291814  Train-Accu: 0.9  Valid-Accu: 0.6572\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.198766  Train-Accu: 1.0  Valid-Accu: 0.6814\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.263825  Train-Accu: 0.925  Valid-Accu: 0.685\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.186367  Train-Accu: 0.975  Valid-Accu: 0.6766\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.23894  Train-Accu: 0.95  Valid-Accu: 0.6742\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.265597  Train-Accu: 0.925  Valid-Accu: 0.6648\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.194481  Train-Accu: 0.975  Valid-Accu: 0.675\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.246753  Train-Accu: 0.975  Valid-Accu: 0.6812\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.202043  Train-Accu: 0.975  Valid-Accu: 0.6704\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.228029  Train-Accu: 0.975  Valid-Accu: 0.6684\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.292811  Train-Accu: 0.9  Valid-Accu: 0.6584\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.242793  Train-Accu: 0.975  Valid-Accu: 0.6746\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.287073  Train-Accu: 0.925  Valid-Accu: 0.6602\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.178162  Train-Accu: 0.975  Valid-Accu: 0.6698\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.229365  Train-Accu: 0.975  Valid-Accu: 0.6752\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.253015  Train-Accu: 0.925  Valid-Accu: 0.671\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.209773  Train-Accu: 0.975  Valid-Accu: 0.6772\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.287045  Train-Accu: 0.925  Valid-Accu: 0.6758\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.177531  Train-Accu: 0.975  Valid-Accu: 0.6694\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.205551  Train-Accu: 0.975  Valid-Accu: 0.6782\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.243656  Train-Accu: 0.95  Valid-Accu: 0.6762\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.20098  Train-Accu: 0.975  Valid-Accu: 0.6696\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.237965  Train-Accu: 0.95  Valid-Accu: 0.6856\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.154668  Train-Accu: 0.975  Valid-Accu: 0.6724\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.207452  Train-Accu: 0.975  Valid-Accu: 0.6608\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.273933  Train-Accu: 0.95  Valid-Accu: 0.669\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.244273  Train-Accu: 0.975  Valid-Accu: 0.6722\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.249655  Train-Accu: 0.95  Valid-Accu: 0.671\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.155549  Train-Accu: 1.0  Valid-Accu: 0.6748\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.182783  Train-Accu: 0.975  Valid-Accu: 0.6668\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.266382  Train-Accu: 0.925  Valid-Accu: 0.6654\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.185993  Train-Accu: 0.975  Valid-Accu: 0.6742\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.259374  Train-Accu: 0.95  Valid-Accu: 0.6612\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.163879  Train-Accu: 0.975  Valid-Accu: 0.6838\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.197282  Train-Accu: 0.975  Valid-Accu: 0.6634\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.259928  Train-Accu: 0.875  Valid-Accu: 0.6642\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.206954  Train-Accu: 0.975  Valid-Accu: 0.6724\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.240851  Train-Accu: 0.95  Valid-Accu: 0.6664\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.145274  Train-Accu: 0.975  Valid-Accu: 0.6828\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.232797  Train-Accu: 0.95  Valid-Accu: 0.671\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.229061  Train-Accu: 0.95  Valid-Accu: 0.664\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.184788  Train-Accu: 0.975  Valid-Accu: 0.6744\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.2148  Train-Accu: 0.925  Valid-Accu: 0.6782\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.15742  Train-Accu: 0.975  Valid-Accu: 0.6768\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.217865  Train-Accu: 0.975  Valid-Accu: 0.6568\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.247645  Train-Accu: 0.925  Valid-Accu: 0.6732\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.205384  Train-Accu: 0.975  Valid-Accu: 0.6682\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.220689  Train-Accu: 0.975  Valid-Accu: 0.6764\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.135254  Train-Accu: 0.975  Valid-Accu: 0.6856\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.21218  Train-Accu: 0.95  Valid-Accu: 0.6546\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.222966  Train-Accu: 0.925  Valid-Accu: 0.6796\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.202081  Train-Accu: 0.975  Valid-Accu: 0.6688\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.218213  Train-Accu: 0.95  Valid-Accu: 0.672\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.147381  Train-Accu: 0.975  Valid-Accu: 0.6714\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.250566  Train-Accu: 0.95  Valid-Accu: 0.6214\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.233738  Train-Accu: 0.925  Valid-Accu: 0.6698\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.185476  Train-Accu: 1.0  Valid-Accu: 0.684\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.236581  Train-Accu: 0.95  Valid-Accu: 0.6688\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.140832  Train-Accu: 0.975  Valid-Accu: 0.6732\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.245175  Train-Accu: 0.975  Valid-Accu: 0.6256\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.262576  Train-Accu: 0.9  Valid-Accu: 0.6746\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.168394  Train-Accu: 1.0  Valid-Accu: 0.6784\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.23196  Train-Accu: 0.95  Valid-Accu: 0.6614\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.168537  Train-Accu: 0.975  Valid-Accu: 0.6732\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.222722  Train-Accu: 0.975  Valid-Accu: 0.6422\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.232481  Train-Accu: 0.925  Valid-Accu: 0.676\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.163918  Train-Accu: 1.0  Valid-Accu: 0.6784\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.206348  Train-Accu: 0.95  Valid-Accu: 0.6668\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.154448  Train-Accu: 0.975  Valid-Accu: 0.6772\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.191429  Train-Accu: 0.975  Valid-Accu: 0.6428\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.21551  Train-Accu: 0.95  Valid-Accu: 0.6838\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.161115  Train-Accu: 1.0  Valid-Accu: 0.6784\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.177375  Train-Accu: 0.975  Valid-Accu: 0.6738\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.122866  Train-Accu: 0.975  Valid-Accu: 0.6758\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.180097  Train-Accu: 0.975  Valid-Accu: 0.6434\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.206624  Train-Accu: 0.95  Valid-Accu: 0.6832\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.146637  Train-Accu: 0.975  Valid-Accu: 0.678\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.193432  Train-Accu: 0.95  Valid-Accu: 0.6716\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.151478  Train-Accu: 0.975  Valid-Accu: 0.6792\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.171387  Train-Accu: 0.975  Valid-Accu: 0.6438\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.20232  Train-Accu: 0.95  Valid-Accu: 0.683\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.141827  Train-Accu: 1.0  Valid-Accu: 0.6736\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.171688  Train-Accu: 0.975  Valid-Accu: 0.6742\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.134507  Train-Accu: 0.975  Valid-Accu: 0.6724\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.167742  Train-Accu: 0.975  Valid-Accu: 0.6316\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.223214  Train-Accu: 0.95  Valid-Accu: 0.6814\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.142433  Train-Accu: 1.0  Valid-Accu: 0.6672\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.157672  Train-Accu: 0.975  Valid-Accu: 0.6698\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.140605  Train-Accu: 0.975  Valid-Accu: 0.68\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.151131  Train-Accu: 0.975  Valid-Accu: 0.6618\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.204684  Train-Accu: 0.95  Valid-Accu: 0.6888\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.121047  Train-Accu: 1.0  Valid-Accu: 0.6808\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.15977  Train-Accu: 0.975  Valid-Accu: 0.6726\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.10955  Train-Accu: 0.975  Valid-Accu: 0.681\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.149836  Train-Accu: 0.975  Valid-Accu: 0.6404\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.205613  Train-Accu: 0.925  Valid-Accu: 0.6856\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.149905  Train-Accu: 1.0  Valid-Accu: 0.6812\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.167515  Train-Accu: 0.95  Valid-Accu: 0.6748\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.125198  Train-Accu: 0.975  Valid-Accu: 0.6764\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.145247  Train-Accu: 0.975  Valid-Accu: 0.6412\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.200948  Train-Accu: 0.95  Valid-Accu: 0.6806\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.146528  Train-Accu: 0.975  Valid-Accu: 0.6758\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.18406  Train-Accu: 0.975  Valid-Accu: 0.6694\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.113713  Train-Accu: 0.975  Valid-Accu: 0.6754\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.168249  Train-Accu: 0.975  Valid-Accu: 0.6472\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.211385  Train-Accu: 0.925  Valid-Accu: 0.6748\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.127932  Train-Accu: 0.975  Valid-Accu: 0.6684\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.166809  Train-Accu: 0.95  Valid-Accu: 0.6674\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.114949  Train-Accu: 0.975  Valid-Accu: 0.676\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.156145  Train-Accu: 0.975  Valid-Accu: 0.6498\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.187602  Train-Accu: 0.95  Valid-Accu: 0.6834\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.133459  Train-Accu: 0.975  Valid-Accu: 0.6766\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.169553  Train-Accu: 1.0  Valid-Accu: 0.664\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.115711  Train-Accu: 0.975  Valid-Accu: 0.6728\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.145475  Train-Accu: 0.975  Valid-Accu: 0.6542\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.2064  Train-Accu: 0.925  Valid-Accu: 0.6712\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.126837  Train-Accu: 1.0  Valid-Accu: 0.6772\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.1881  Train-Accu: 0.975  Valid-Accu: 0.6612\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.119032  Train-Accu: 0.975  Valid-Accu: 0.6712\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.158999  Train-Accu: 0.975  Valid-Accu: 0.6408\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.20084  Train-Accu: 0.925  Valid-Accu: 0.673\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.120987  Train-Accu: 1.0  Valid-Accu: 0.67\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.161311  Train-Accu: 0.975  Valid-Accu: 0.673\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.102315  Train-Accu: 1.0  Valid-Accu: 0.6806\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.164347  Train-Accu: 0.975  Valid-Accu: 0.6302\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.20009  Train-Accu: 0.95  Valid-Accu: 0.6752\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.131787  Train-Accu: 0.975  Valid-Accu: 0.6794\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.161084  Train-Accu: 0.975  Valid-Accu: 0.6614\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.120677  Train-Accu: 0.975  Valid-Accu: 0.6852\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.155825  Train-Accu: 0.975  Valid-Accu: 0.6638\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.235195  Train-Accu: 0.925  Valid-Accu: 0.6698\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.148812  Train-Accu: 0.975  Valid-Accu: 0.6698\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.154451  Train-Accu: 0.975  Valid-Accu: 0.6718\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.12184  Train-Accu: 0.975  Valid-Accu: 0.6856\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.160342  Train-Accu: 0.975  Valid-Accu: 0.6736\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.193041  Train-Accu: 0.95  Valid-Accu: 0.6692\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.145576  Train-Accu: 0.975  Valid-Accu: 0.6738\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.157823  Train-Accu: 0.975  Valid-Accu: 0.6458\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.0940802  Train-Accu: 0.975  Valid-Accu: 0.6888\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.139633  Train-Accu: 0.975  Valid-Accu: 0.666\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.203996  Train-Accu: 0.925  Valid-Accu: 0.6788\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.138844  Train-Accu: 0.975  Valid-Accu: 0.68\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.134168  Train-Accu: 1.0  Valid-Accu: 0.6612\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.102765  Train-Accu: 0.975  Valid-Accu: 0.6856\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.152004  Train-Accu: 0.975  Valid-Accu: 0.672\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.189017  Train-Accu: 0.925  Valid-Accu: 0.6818\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.125842  Train-Accu: 1.0  Valid-Accu: 0.6742\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.162672  Train-Accu: 0.975  Valid-Accu: 0.6438\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.106285  Train-Accu: 0.975  Valid-Accu: 0.6832\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.125593  Train-Accu: 1.0  Valid-Accu: 0.6722\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.193775  Train-Accu: 0.925  Valid-Accu: 0.6784\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.11807  Train-Accu: 1.0  Valid-Accu: 0.6822\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.1669  Train-Accu: 0.975  Valid-Accu: 0.663\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.113438  Train-Accu: 0.975  Valid-Accu: 0.6824\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.131352  Train-Accu: 0.975  Valid-Accu: 0.6744\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.191354  Train-Accu: 0.925  Valid-Accu: 0.6692\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.116611  Train-Accu: 1.0  Valid-Accu: 0.666\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.163311  Train-Accu: 0.975  Valid-Accu: 0.658\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.100923  Train-Accu: 1.0  Valid-Accu: 0.6764\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.130617  Train-Accu: 1.0  Valid-Accu: 0.673\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.168246  Train-Accu: 0.95  Valid-Accu: 0.668\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.10891  Train-Accu: 1.0  Valid-Accu: 0.6756\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.154669  Train-Accu: 0.975  Valid-Accu: 0.6572\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.095983  Train-Accu: 1.0  Valid-Accu: 0.6772\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.127501  Train-Accu: 1.0  Valid-Accu: 0.6702\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.175348  Train-Accu: 0.925  Valid-Accu: 0.6712\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.102037  Train-Accu: 1.0  Valid-Accu: 0.6598\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.139384  Train-Accu: 0.975  Valid-Accu: 0.6596\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.0818902  Train-Accu: 0.975  Valid-Accu: 0.6804\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.12379  Train-Accu: 0.975  Valid-Accu: 0.677\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.164464  Train-Accu: 0.95  Valid-Accu: 0.6688\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.104077  Train-Accu: 1.0  Valid-Accu: 0.6728\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.152693  Train-Accu: 1.0  Valid-Accu: 0.6728\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.0930443  Train-Accu: 0.975  Valid-Accu: 0.6844\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.110778  Train-Accu: 0.975  Valid-Accu: 0.6784\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.165508  Train-Accu: 1.0  Valid-Accu: 0.6758\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.0971089  Train-Accu: 1.0  Valid-Accu: 0.6786\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.138185  Train-Accu: 0.975  Valid-Accu: 0.658\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.111667  Train-Accu: 1.0  Valid-Accu: 0.6788\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.118631  Train-Accu: 0.975  Valid-Accu: 0.668\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.171599  Train-Accu: 0.975  Valid-Accu: 0.6728\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.119917  Train-Accu: 0.975  Valid-Accu: 0.655\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.140187  Train-Accu: 1.0  Valid-Accu: 0.6644\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.123026  Train-Accu: 0.975  Valid-Accu: 0.675\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.109767  Train-Accu: 0.975  Valid-Accu: 0.6734\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.167165  Train-Accu: 0.95  Valid-Accu: 0.6636\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.110491  Train-Accu: 0.975  Valid-Accu: 0.6656\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.115745  Train-Accu: 1.0  Valid-Accu: 0.6668\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.113386  Train-Accu: 0.975  Valid-Accu: 0.6768\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.107862  Train-Accu: 1.0  Valid-Accu: 0.681\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.14825  Train-Accu: 0.975  Valid-Accu: 0.6642\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.11035  Train-Accu: 1.0  Valid-Accu: 0.6636\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.125659  Train-Accu: 0.975  Valid-Accu: 0.6634\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.114491  Train-Accu: 0.975  Valid-Accu: 0.676\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.106894  Train-Accu: 0.975  Valid-Accu: 0.6694\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.14522  Train-Accu: 0.975  Valid-Accu: 0.6692\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.104882  Train-Accu: 1.0  Valid-Accu: 0.6708\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.123619  Train-Accu: 1.0  Valid-Accu: 0.6672\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.122146  Train-Accu: 1.0  Valid-Accu: 0.6658\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.10412  Train-Accu: 1.0  Valid-Accu: 0.6804\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.14773  Train-Accu: 1.0  Valid-Accu: 0.6694\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.107986  Train-Accu: 1.0  Valid-Accu: 0.6674\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.129844  Train-Accu: 1.0  Valid-Accu: 0.672\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.081173  Train-Accu: 0.975  Valid-Accu: 0.6698\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.0903049  Train-Accu: 1.0  Valid-Accu: 0.6698\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.142595  Train-Accu: 1.0  Valid-Accu: 0.6616\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.102487  Train-Accu: 0.975  Valid-Accu: 0.6672\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.129091  Train-Accu: 0.975  Valid-Accu: 0.6756\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.100058  Train-Accu: 1.0  Valid-Accu: 0.6722\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.112619  Train-Accu: 1.0  Valid-Accu: 0.6636\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.156492  Train-Accu: 1.0  Valid-Accu: 0.6588\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.0817557  Train-Accu: 1.0  Valid-Accu: 0.6698\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.120374  Train-Accu: 1.0  Valid-Accu: 0.6706\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.0925822  Train-Accu: 0.975  Valid-Accu: 0.676\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.0999073  Train-Accu: 1.0  Valid-Accu: 0.6756\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.127052  Train-Accu: 1.0  Valid-Accu: 0.6676\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.0880672  Train-Accu: 1.0  Valid-Accu: 0.6592\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.13443  Train-Accu: 0.975  Valid-Accu: 0.6672\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.0944383  Train-Accu: 0.975  Valid-Accu: 0.6736\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.10913  Train-Accu: 0.975  Valid-Accu: 0.6736\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.13911  Train-Accu: 1.0  Valid-Accu: 0.6732\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.0822661  Train-Accu: 1.0  Valid-Accu: 0.6688\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.111121  Train-Accu: 1.0  Valid-Accu: 0.6712\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.078081  Train-Accu: 0.975  Valid-Accu: 0.6696\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.0968253  Train-Accu: 1.0  Valid-Accu: 0.6804\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.141638  Train-Accu: 1.0  Valid-Accu: 0.6616\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.0833718  Train-Accu: 1.0  Valid-Accu: 0.6742\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.112514  Train-Accu: 1.0  Valid-Accu: 0.6762\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.0908297  Train-Accu: 0.975  Valid-Accu: 0.672\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.130186  Train-Accu: 0.95  Valid-Accu: 0.6784\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.129308  Train-Accu: 1.0  Valid-Accu: 0.6698\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.0769778  Train-Accu: 1.0  Valid-Accu: 0.6776\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.105564  Train-Accu: 1.0  Valid-Accu: 0.6788\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.0994562  Train-Accu: 0.975  Valid-Accu: 0.6684\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.102024  Train-Accu: 1.0  Valid-Accu: 0.6794\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.140353  Train-Accu: 0.975  Valid-Accu: 0.6524\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.0980577  Train-Accu: 1.0  Valid-Accu: 0.6612\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.106047  Train-Accu: 1.0  Valid-Accu: 0.6756\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.0816332  Train-Accu: 0.975  Valid-Accu: 0.6778\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.113056  Train-Accu: 0.95  Valid-Accu: 0.6798\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.130036  Train-Accu: 0.975  Valid-Accu: 0.6616\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.0861039  Train-Accu: 1.0  Valid-Accu: 0.6746\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.114224  Train-Accu: 0.975  Valid-Accu: 0.6736\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.0750933  Train-Accu: 0.975  Valid-Accu: 0.6746\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.12968  Train-Accu: 0.975  Valid-Accu: 0.6726\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.161988  Train-Accu: 0.95  Valid-Accu: 0.6616\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.0722317  Train-Accu: 1.0  Valid-Accu: 0.6734\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.1071  Train-Accu: 0.975  Valid-Accu: 0.6708\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.0869616  Train-Accu: 0.975  Valid-Accu: 0.671\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.120475  Train-Accu: 0.95  Valid-Accu: 0.6718\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.138557  Train-Accu: 0.95  Valid-Accu: 0.6676\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.0696582  Train-Accu: 1.0  Valid-Accu: 0.6756\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.110509  Train-Accu: 0.975  Valid-Accu: 0.6712\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.110803  Train-Accu: 0.95  Valid-Accu: 0.6706\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.110383  Train-Accu: 0.975  Valid-Accu: 0.6744\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.144865  Train-Accu: 0.975  Valid-Accu: 0.6674\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.0719996  Train-Accu: 1.0  Valid-Accu: 0.671\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.105295  Train-Accu: 0.975  Valid-Accu: 0.6748\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.0759039  Train-Accu: 0.975  Valid-Accu: 0.68\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.0953028  Train-Accu: 1.0  Valid-Accu: 0.6816\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.130639  Train-Accu: 1.0  Valid-Accu: 0.6758\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.0680128  Train-Accu: 1.0  Valid-Accu: 0.6782\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.0913729  Train-Accu: 1.0  Valid-Accu: 0.6796\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.074125  Train-Accu: 0.975  Valid-Accu: 0.6718\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.107866  Train-Accu: 1.0  Valid-Accu: 0.6728\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.11722  Train-Accu: 1.0  Valid-Accu: 0.6668\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.0912607  Train-Accu: 1.0  Valid-Accu: 0.6732\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.100615  Train-Accu: 0.975  Valid-Accu: 0.6792\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.0647028  Train-Accu: 1.0  Valid-Accu: 0.6816\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.67470703125\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDo7NzMzMzDIHx2ZmZmZm2f8HiAOE8WAZ/YkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77472d6e48>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
